{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nw57nbNblIhD",
        "CDA6mnjIx2Zn",
        "RKJ3-WTVWafD",
        "yu8q0t3tYymV",
        "KiKYJrfXcZF3"
      ],
      "mount_file_id": "1-nvAbPZ2BqbuHwmpgXSXvHRnrE-ruLGp",
      "authorship_tag": "ABX9TyMVb4E+u41ZWbvN+41j0Flt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrinankManakin/Text-to-sql-Llama-2-model/blob/main/Copy_of_Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run these first\n"
      ],
      "metadata": {
        "id": "-NnXlaNXLXpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MrinankManakin/Text-to-sql-Llama-2-model.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2m7OzIcPVsD",
        "outputId": "cf8677fc-fd31-4e07-b99b-24e3a9924414"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Text-to-sql-Llama-2-model' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "QalUY53OQPmj",
        "outputId": "d63cb6ff-a04f-407b-e173-b884e1b58b12"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-3-746ab54d8bd4>, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-746ab54d8bd4>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    cd Text-to-sql-Llama-2-model\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python\n",
        "!pip install pprintpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY4G3P_pH_-a",
        "outputId": "23a7652a-b921-4109-b6dc-7e74d6b91cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.81.tar.gz (50.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.81-cp310-cp310-linux_x86_64.whl size=2762347 sha256=68cd538fe0d3166c0db0b33759050df3a42a22fe860b10ddeff5b2c4e8e9f857\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/4a/7f/592e73429da89e4086bbd65cda7197ab53c403c17d386894cd\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.81\n",
            "Collecting pprintpp\n",
            "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: pprintpp\n",
            "Successfully installed pprintpp-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NanPRuFuD_S5"
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "import pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRT1_vAgJXii",
        "outputId": "049e0100-cf1d-4d5e-a29f-4dc3b10c47c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/Final Working LLM Code/Llama-2-FT.gguf\"\n",
        "absolute_model_path = os.path.abspath(model_path)\n",
        "print(absolute_model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL_93P0xPC5T",
        "outputId": "4d43e4be-b913-4d6e-a31a-baab04910dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Final Working LLM Code/Llama-2-FT.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now your Google Drive is mounted, you can access files from it\n",
        "model_path = \"/content/drive/MyDrive/Final Working LLM Code/Llama-2-FT.gguf\"\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    chat_format=\"chatml\"\n",
        ")"
      ],
      "metadata": {
        "id": "X-JpimZwEXqF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca11b39c-6a35-4e1f-d3a5-b25220e43686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /content/drive/MyDrive/Final Working LLM Code/Llama-2-FT.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q8_0:  226 tensors\n",
            "llm_load_vocab: special tokens cache size = 259\n",
            "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  6828.64 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
            "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Attempt"
      ],
      "metadata": {
        "id": "nw57nbNblIhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "znIGb5xmMZg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read dataset\n",
        "dataset_path = \"/content/drive/MyDrive/new code/employee_churn_data.csv\"\n",
        "if not os.path.exists(dataset_path):\n",
        "    raise FileNotFoundError(f\"File '{dataset_path}' not found\")\n",
        "\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "# Define SQL schema based on dataset variables\n",
        "schema = \"\"\"\n",
        "CREATE TABLE employee_churn_data (\n",
        "    department TEXT,\n",
        "    promoted INTEGER,\n",
        "    review INTEGER,\n",
        "    projects INTEGER,\n",
        "    salary FLOAT,\n",
        "    tenure INTEGER,\n",
        "    satisfaction FLOAT,\n",
        "    bonus FLOAT,\n",
        "    avg_hrs_month FLOAT,\n",
        "    left INTEGER\n",
        ")\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "OCVFJRhJMaRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = \"CREATE TABLE head (age INTEGER)\"\n",
        "question = \"Give me the SQL query to get the departments with Low salary\"\n",
        "\n",
        "prompt = f\"\"\"Schema:{schema}\\nQuestion:{question}\"\"\"\n",
        "\n",
        "intial_string = f\"\"\"Generate a precise SQL query from the given english question in correct SQL format based on the provided English question and schema.\n",
        "Args:\n",
        "  question: The question for the SQL generation task.\n",
        "  schema: The SQL Schema that provides context for the task.\n",
        "Returns:\n",
        "  Returns the generated query\"\"\"\n",
        "\n",
        "response = llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": intial_string,\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ],\n",
        "    response_format={\n",
        "        \"type\": \"json_object\",\n",
        "    },\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "pprint.pprint(response[\"choices\"][0][\"message\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbAzGfmAIu9b",
        "outputId": "3ec106ed-c4e8-442b-9328-a3f988900ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "from_string grammar:\n",
            "root ::= object \n",
            "object ::= [{] ws object_11 [}] ws \n",
            "value ::= object | array | string | number | value_6 ws \n",
            "array ::= [[] ws array_15 []] ws \n",
            "string ::= [\"] string_18 [\"] ws \n",
            "number ::= number_19 number_25 number_29 ws \n",
            "value_6 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
            "ws ::= ws_31 \n",
            "object_8 ::= string [:] ws value object_10 \n",
            "object_9 ::= [,] ws string [:] ws value \n",
            "object_10 ::= object_9 object_10 | \n",
            "object_11 ::= object_8 | \n",
            "array_12 ::= value array_14 \n",
            "array_13 ::= [,] ws value \n",
            "array_14 ::= array_13 array_14 | \n",
            "array_15 ::= array_12 | \n",
            "string_16 ::= [^\"\\<U+0000>-<U+001F>] | [\\] string_17 \n",
            "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
            "string_18 ::= string_16 string_18 | \n",
            "number_19 ::= number_20 number_21 \n",
            "number_20 ::= [-] | \n",
            "number_21 ::= [0-9] | [1-9] number_22 \n",
            "number_22 ::= [0-9] number_22 | \n",
            "number_23 ::= [.] number_24 \n",
            "number_24 ::= [0-9] number_24 | [0-9] \n",
            "number_25 ::= number_23 | \n",
            "number_26 ::= [eE] number_27 number_28 \n",
            "number_27 ::= [-+] | \n",
            "number_28 ::= [0-9] number_28 | [0-9] \n",
            "number_29 ::= number_26 | \n",
            "ws_30 ::= [ <U+0009><U+000A>] ws \n",
            "ws_31 ::= ws_30 | \n",
            "\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   77469.63 ms\n",
            "llama_print_timings:      sample time =     318.80 ms /    19 runs   (   16.78 ms per token,    59.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =   25528.25 ms /    33 tokens (  773.58 ms per token,     1.29 tokens per second)\n",
            "llama_print_timings:        eval time =   19306.22 ms /    18 runs   ( 1072.57 ms per token,     0.93 tokens per second)\n",
            "llama_print_timings:       total time =   45261.27 ms /    51 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'content': '{ \"SELECT\" : \"SELECT age FROM head WHERE age < 30\"}\\n',\n",
            " 'role': 'assistant'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Guardrails for output parsing\n"
      ],
      "metadata": {
        "id": "CDA6mnjIx2Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install llama-index-llms-openai\n",
        "# %pip install llama-index-output-parsers-guardrails"
      ],
      "metadata": {
        "id": "MrNz5IaF7SUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e825c5f-ea24-4b3e-d674-8875619a5f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-llms-openai\n",
            "  Downloading llama_index_llms_openai-0.1.15-py3-none-any.whl (10 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.24 (from llama-index-llms-openai)\n",
            "  Downloading llama_index_core-0.10.29-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.9.3)\n",
            "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2023.6.0)\n",
            "Collecting httpx (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llamaindex-py-client<0.2.0,>=0.1.18 (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading llamaindex_py_client-0.1.18-py3-none-any.whl (136 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.1/136.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.25.2)\n",
            "Collecting openai>=1.1.0 (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading openai-1.20.0-py3-none-any.whl (292 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (4.11.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.14.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (4.0.3)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.6.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.2.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (24.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.16.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.16.0)\n",
            "Installing collected packages: dirtyjson, mypy-extensions, marshmallow, h11, deprecated, typing-inspect, tiktoken, httpcore, httpx, dataclasses-json, openai, llamaindex-py-client, llama-index-core, llama-index-llms-openai\n",
            "Successfully installed dataclasses-json-0.6.4 deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 llama-index-core-0.10.29 llama-index-llms-openai-0.1.15 llamaindex-py-client-0.1.18 marshmallow-3.21.1 mypy-extensions-1.0.0 openai-1.20.0 tiktoken-0.6.0 typing-inspect-0.9.0\n",
            "Collecting llama-index-output-parsers-guardrails\n",
            "  Downloading llama_index_output_parsers_guardrails-0.1.3-py3-none-any.whl (2.5 kB)\n",
            "Collecting guardrails-ai<0.5.0,>=0.4.1 (from llama-index-output-parsers-guardrails)\n",
            "  Downloading guardrails_ai-0.4.3-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.5/190.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-output-parsers-guardrails) (0.10.29)\n",
            "Collecting coloredlogs<16.0.0,>=15.0.1 (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting griffe<0.37.0,>=0.36.9 (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading griffe-0.36.9-py3-none-any.whl (111 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting guardrails-api-client<0.2.0,>=0.1.1 (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading guardrails_api_client-0.1.1-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.4/67.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jwt<2.0.0,>=1.3.1 (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading jwt-1.3.1-py3-none-any.whl (18 kB)\n",
            "Collecting langchain-core<0.2.0,>=0.1.18 (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading langchain_core-0.1.43-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.1/289.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml<5.0.0,>=4.9.3 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (4.9.4)\n",
            "Requirement already satisfied: openai<2 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (1.20.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.20.0 (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.20.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.20.0 (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.20.0-py3-none-any.whl (16 kB)\n",
            "Collecting opentelemetry-sdk==1.20.0 (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading opentelemetry_sdk-1.20.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.9/103.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3.0,>=1.10.9 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (2.6.4)\n",
            "Collecting pydash<8.0.0,>=7.0.6 (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading pydash-7.0.7-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.3/110.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (2.8.2)\n",
            "Requirement already satisfied: regex<2024.0.0,>=2023.10.3 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (2023.12.25)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (2.31.0)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.6.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (13.7.1)\n",
            "Collecting rstr<4.0.0,>=3.2.2 (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading rstr-3.2.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tenacity>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (8.2.3)\n",
            "Collecting tiktoken<0.6.0,>=0.5.1 (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer[all]<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (0.9.4)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (4.11.0)\n",
            "Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (1.2.14)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (1.63.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (1.62.1)\n",
            "Collecting opentelemetry-api~=1.15 (from opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-common==1.20.0 (from opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.20.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.20.0 (from opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading opentelemetry_proto-1.20.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api~=1.15 (from opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading opentelemetry_api-1.20.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-semantic-conventions==0.41b0 (from opentelemetry-sdk==1.20.0->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading opentelemetry_semantic_conventions-0.41b0-py3-none-any.whl (26 kB)\n",
            "Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api~=1.15->opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: protobuf<5.0,>=3.19 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-proto==1.20.0->opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (3.20.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (3.9.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (0.6.4)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (0.27.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (0.1.18)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (9.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (4.66.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (1.14.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (4.0.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs<16.0.0,>=15.0.1->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorama>=0.4 (from griffe<0.37.0,>=0.36.9->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (0.14.0)\n",
            "Requirement already satisfied: cryptography!=3.4.0,>=3.1 in /usr/local/lib/python3.10/dist-packages (from jwt<2.0.0,>=1.3.1->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (42.0.5)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.2.0,>=0.1.18->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.0 (from langchain-core<0.2.0,>=0.1.18->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading langsmith-0.1.48-py3-none-any.whl (113 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.7/113.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.18->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (1.4.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (1.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=1.10.9->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=1.10.9->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (2.16.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.8.2->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (2.0.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.6.0->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.6.0->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (3.0.3)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<0.10.0,>=0.9.0->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (3.21.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-output-parsers-guardrails) (1.2.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography!=3.4.0,>=3.1->jwt<2.0.0,>=1.3.1->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (1.16.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.18->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.18->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails)\n",
            "  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.6.0->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (0.1.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography!=3.4.0,>=3.1->jwt<2.0.0,>=1.3.1->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (2.22)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api~=1.15->opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai<0.5.0,>=0.4.1->llama-index-output-parsers-guardrails) (3.18.1)\n",
            "Installing collected packages: shellingham, rstr, pydash, packaging, orjson, opentelemetry-semantic-conventions, opentelemetry-proto, jsonpointer, importlib-metadata, humanfriendly, colorama, backoff, tiktoken, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, jsonpatch, griffe, coloredlogs, opentelemetry-sdk, langsmith, jwt, guardrails-api-client, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, langchain-core, guardrails-ai, llama-index-output-parsers-guardrails\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "  Attempting uninstall: tiktoken\n",
            "    Found existing installation: tiktoken 0.6.0\n",
            "    Uninstalling tiktoken-0.6.0:\n",
            "      Successfully uninstalled tiktoken-0.6.0\n",
            "Successfully installed backoff-2.2.1 colorama-0.4.6 coloredlogs-15.0.1 griffe-0.36.9 guardrails-ai-0.4.3 guardrails-api-client-0.1.1 humanfriendly-10.0 importlib-metadata-6.11.0 jsonpatch-1.33 jsonpointer-2.4 jwt-1.3.1 langchain-core-0.1.43 langsmith-0.1.48 llama-index-output-parsers-guardrails-0.1.3 opentelemetry-api-1.20.0 opentelemetry-exporter-otlp-proto-common-1.20.0 opentelemetry-exporter-otlp-proto-grpc-1.20.0 opentelemetry-exporter-otlp-proto-http-1.20.0 opentelemetry-proto-1.20.0 opentelemetry-sdk-1.20.0 opentelemetry-semantic-conventions-0.41b0 orjson-3.10.1 packaging-23.2 pydash-7.0.7 rstr-3.2.2 shellingham-1.5.4 tiktoken-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install guardrails-ai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe-ztDGUyZYQ",
        "outputId": "763c7fa6-c80e-4d5d-ad3f-3e24e7b9f73d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: guardrails-ai in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: coloredlogs<16.0.0,>=15.0.1 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (15.0.1)\n",
            "Requirement already satisfied: griffe<0.37.0,>=0.36.9 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (0.36.9)\n",
            "Requirement already satisfied: guardrails-api-client<0.2.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (0.1.1)\n",
            "Requirement already satisfied: jwt<2.0.0,>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (1.3.1)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.18 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (0.1.43)\n",
            "Requirement already satisfied: lxml<5.0.0,>=4.9.3 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (4.9.4)\n",
            "Requirement already satisfied: openai<2 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (1.20.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.20.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (1.20.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.20.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (1.20.0)\n",
            "Requirement already satisfied: opentelemetry-sdk==1.20.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (1.20.0)\n",
            "Requirement already satisfied: pydantic<3.0,>=1.10.9 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (2.6.4)\n",
            "Requirement already satisfied: pydash<8.0.0,>=7.0.6 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (7.0.7)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (2.8.2)\n",
            "Requirement already satisfied: regex<2024.0.0,>=2023.10.3 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (2023.12.25)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (2.31.0)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.6.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (13.7.1)\n",
            "Requirement already satisfied: rstr<4.0.0,>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (3.2.2)\n",
            "Requirement already satisfied: tenacity>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (8.2.3)\n",
            "Requirement already satisfied: tiktoken<0.6.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (0.5.2)\n",
            "Requirement already satisfied: typer[all]<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (0.9.4)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai) (4.11.0)\n",
            "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai) (2.2.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai) (1.2.14)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai) (1.63.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai) (1.62.1)\n",
            "Requirement already satisfied: opentelemetry-api~=1.15 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai) (1.20.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.20.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai) (1.20.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.20.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai) (1.20.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.41b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk==1.20.0->guardrails-ai) (0.41b0)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api~=1.15->opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai) (6.11.0)\n",
            "Requirement already satisfied: protobuf<5.0,>=3.19 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-proto==1.20.0->opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai) (3.20.3)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs<16.0.0,>=15.0.1->guardrails-ai) (10.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.10/dist-packages (from griffe<0.37.0,>=0.36.9->guardrails-ai) (0.4.6)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-api-client<0.2.0,>=0.1.1->guardrails-ai) (0.27.0)\n",
            "Requirement already satisfied: attrs>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-api-client<0.2.0,>=0.1.1->guardrails-ai) (23.2.0)\n",
            "Requirement already satisfied: cryptography!=3.4.0,>=3.1 in /usr/local/lib/python3.10/dist-packages (from jwt<2.0.0,>=1.3.1->guardrails-ai) (42.0.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.18->guardrails-ai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.18->guardrails-ai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.18->guardrails-ai) (0.1.48)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.18->guardrails-ai) (23.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2->guardrails-ai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2->guardrails-ai) (1.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2->guardrails-ai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2->guardrails-ai) (4.66.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=1.10.9->guardrails-ai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=1.10.9->guardrails-ai) (2.16.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.8.2->guardrails-ai) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->guardrails-ai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->guardrails-ai) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->guardrails-ai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->guardrails-ai) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.6.0->guardrails-ai) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.6.0->guardrails-ai) (2.16.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<0.10.0,>=0.9.0->guardrails-ai) (8.1.7)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<0.10.0,>=0.9.0->guardrails-ai) (1.5.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2->guardrails-ai) (1.2.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography!=3.4.0,>=3.1->jwt<2.0.0,>=1.3.1->guardrails-ai) (1.16.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai) (1.14.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.20.0->guardrails-api-client<0.2.0,>=0.1.1->guardrails-ai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.20.0->guardrails-api-client<0.2.0,>=0.1.1->guardrails-ai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.18->guardrails-ai) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.18->guardrails-ai) (3.10.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.6.0->guardrails-ai) (0.1.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography!=3.4.0,>=3.1->jwt<2.0.0,>=1.3.1->guardrails-ai) (2.22)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api~=1.15->opentelemetry-exporter-otlp-proto-grpc==1.20.0->guardrails-ai) (3.18.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir -p 'data/paul_graham/'\n",
        "# !curl 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' > 'data/paul_graham/paul_graham_essay.txt'"
      ],
      "metadata": {
        "id": "FArc0s1By71u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7tlxPGwy8an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trying the Model on a New Dataset**"
      ],
      "metadata": {
        "id": "RKJ3-WTVWafD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SndvoEzWYx1",
        "outputId": "aad82a84-98c0-466c-a271-bbdffbfc22f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.64)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2kppYC5jrIE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from llama_cpp import Llama\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Read dataset\n",
        "dataset_path = \"/content/drive/MyDrive/new code/Salary Data.csv\"\n",
        "if not os.path.exists(dataset_path):\n",
        "    raise FileNotFoundError(f\"File '{dataset_path}' not found\")\n",
        "\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Define SQL schema based on dataset variables\n",
        "schema = \"\"\"\n",
        "CREATE TABLE SalaryData (\n",
        "    Age INTEGER,\n",
        "    Gender TEXT,\n",
        "    Education_Level TEXT,\n",
        "    Job_Title TEXT,\n",
        "    Years_of_Experience INTEGER,\n",
        "    Salary FLOAT\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# User prompt\n",
        "user_prompt = \"Generate a SQL query to count more than 4 years of experience from my 'Years_of_experience' table.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Set path to the LLM model\n",
        "model_path = \"/content/drive/MyDrive/new code/Llama-2-FT.gguf\"\n",
        "absolute_model_path = os.path.abspath(model_path)\n",
        "\n",
        "# Initialize LLM\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    chat_format=\"chatml\"\n",
        ")\n",
        "\n",
        "# Generate response\n",
        "response = llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Generating SQL query based on dataset.\"},\n",
        "        {\"role\": \"system\", \"content\": user_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ],\n",
        "    response_format={\"type\": \"json_object\"},\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "# Extract and print the generated SQL query\n",
        "generated_query = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "print(\"Generated SQL query:\")\n",
        "print(generated_query)\n"
      ],
      "metadata": {
        "id": "7rKUvzEZOm8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01c760a9-384e-4353-a69d-02c396d1ec95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /content/drive/MyDrive/new code/Llama-2-FT.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q8_0:  226 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  6828.64 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
            "from_string grammar:\n",
            "root ::= object \n",
            "object ::= [{] ws object_11 [}] ws \n",
            "value ::= object | array | string | number | value_6 ws \n",
            "array ::= [[] ws array_15 []] ws \n",
            "string ::= [\"] string_18 [\"] ws \n",
            "number ::= number_19 number_25 number_29 ws \n",
            "value_6 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
            "ws ::= ws_31 \n",
            "object_8 ::= string [:] ws value object_10 \n",
            "object_9 ::= [,] ws string [:] ws value \n",
            "object_10 ::= object_9 object_10 | \n",
            "object_11 ::= object_8 | \n",
            "array_12 ::= value array_14 \n",
            "array_13 ::= [,] ws value \n",
            "array_14 ::= array_13 array_14 | \n",
            "array_15 ::= array_12 | \n",
            "string_16 ::= [^\"\\<U+0000>-<U+001F>] | [\\] string_17 \n",
            "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
            "string_18 ::= string_16 string_18 | \n",
            "number_19 ::= number_20 number_21 \n",
            "number_20 ::= [-] | \n",
            "number_21 ::= [0-9] | [1-9] number_22 \n",
            "number_22 ::= [0-9] number_22 | \n",
            "number_23 ::= [.] number_24 \n",
            "number_24 ::= [0-9] number_24 | [0-9] \n",
            "number_25 ::= number_23 | \n",
            "number_26 ::= [eE] number_27 number_28 \n",
            "number_27 ::= [-+] | \n",
            "number_28 ::= [0-9] number_28 | [0-9] \n",
            "number_29 ::= number_26 | \n",
            "ws_30 ::= [ <U+0009><U+000A>] ws \n",
            "ws_31 ::= ws_30 | \n",
            "\n",
            "\n",
            "llama_print_timings:        load time =   35793.67 ms\n",
            "llama_print_timings:      sample time =     451.25 ms /    31 runs   (   14.56 ms per token,    68.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =   35792.31 ms /    80 tokens (  447.40 ms per token,     2.24 tokens per second)\n",
            "llama_print_timings:        eval time =   25366.58 ms /    30 runs   (  845.55 ms per token,     1.18 tokens per second)\n",
            "llama_print_timings:       total time =   61785.20 ms /   110 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated SQL query:\n",
            "{\n",
            "    \"query\": \"SELECT COUNT(*) FROM table_name WHERE Years_of_expériencé > 4\"\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDoM0hOYLQ9s",
        "outputId": "0a0c9597-cabd-4ea8-a081-b97e14960a34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-93087599-f15e-4259-ab7a-0ab72657f25f', 'object': 'chat.completion', 'created': 1714639367, 'model': '/content/drive/MyDrive/new code/Llama-2-FT.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '{\\n    \"query\": \"SELECT COUNT(*) FROM table_name WHERE Years_of_expériencé > 4\"\\n}\\n'}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 80, 'completion_tokens': 30, 'total_tokens': 110}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract and print the generated SQL query\n",
        "generated_query = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "print(\"Generated SQL query:\")\n",
        "print(generated_query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV_7WMYcMXx1",
        "outputId": "b86e3c61-b62b-4f56-fd6d-4281702ab3b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated SQL query:\n",
            "{\n",
            "    \"query\": \"SELECT COUNT(*) FROM table_name WHERE Years_of_expériencé > 4\"\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying to store the SCHEMA of the dataset as a JSON file for the LLM to access and return better results"
      ],
      "metadata": {
        "id": "yu8q0t3tYymV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  import pandas as pd\n",
        "\n",
        "# Load the dataset from CSV file\n",
        "dataset_path = \"/content/drive/MyDrive/new code/Salary Data.csv\"  # Replace \"your_dataset.csv\" with the path to your dataset\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Define the schema for each table in your dataset\n",
        "# Replace the column names and data types with those from your dataset\n",
        "\n",
        "# Employees table schema\n",
        "employees_schema = {\n",
        "    \"EmployeeID\": \"int\",\n",
        "    \"FirstName\": \"str\",\n",
        "    \"LastName\": \"str\",\n",
        "    \"DepartmentID\": \"int\",\n",
        "    \"Years_of_Experience\": \"int\",\n",
        "    # Add more columns as needed\n",
        "}\n",
        "\n",
        "# Departments table schema\n",
        "departments_schema = {\n",
        "    \"DepartmentID\": \"int\",\n",
        "    \"DepartmentName\": \"str\",\n",
        "    # Add more columns as needed\n",
        "}\n",
        "\n",
        "# Salaries table schema\n",
        "salaries_schema = {\n",
        "    \"EmployeeID\": \"int\",\n",
        "    \"Salary\": \"float\",\n",
        "    # Add more columns as needed\n",
        "}\n",
        "\n",
        "# Print the defined schemas\n",
        "print(\"Employees table schema:\")\n",
        "print(employees_schema)\n",
        "print(\"\\nDepartments table schema:\")\n",
        "print(departments_schema)\n",
        "print(\"\\nSalaries table schema:\")\n",
        "print(salaries_schema)\n"
      ],
      "metadata": {
        "id": "2_uG4egiPoQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed2b317-86bd-46d4-bccd-5e52f329e18d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employees table schema:\n",
            "{'EmployeeID': 'int', 'FirstName': 'str', 'LastName': 'str', 'DepartmentID': 'int', 'Years_of_Experience': 'int'}\n",
            "\n",
            "Departments table schema:\n",
            "{'DepartmentID': 'int', 'DepartmentName': 'str'}\n",
            "\n",
            "Salaries table schema:\n",
            "{'EmployeeID': 'int', 'Salary': 'float'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Load the dataset from CSV file\n",
        "dataset_path = \"/content/drive/MyDrive/new code/Salary Data.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Define the schema for each table in your dataset\n",
        "# Replace the column names and data types with those from your dataset\n",
        "\n",
        "# Employees table schema\n",
        "employees_schema = {\n",
        "    \"Age\": \"int\",\n",
        "    \"Gender\": \"str\",\n",
        "    \"Education_Level\": \"str\",\n",
        "    \"Job_Title\": \"str\",\n",
        "    \"Years_of_Experience\": \"int\",\n",
        "    \"Salary\": \"float\"\n",
        "    # Add more columns as needed\n",
        "}\n",
        "\n",
        "# Print the defined schema\n",
        "print(\"Employees table schema:\")\n",
        "print(employees_schema)\n",
        "\n",
        "# Save the schema to a JSON file\n",
        "with open('dataset_schema.json', 'w') as f:\n",
        "    json.dump(employees_schema, f)\n",
        "\n",
        "print(\"Dataset schema saved to 'dataset_schema.json'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCgQaFVTpmfE",
        "outputId": "d7631fe0-5adf-441d-ee10-a02aa32ee34c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employees table schema:\n",
            "{'Age': 'int', 'Gender': 'str', 'Education_Level': 'str', 'Job_Title': 'str', 'Years_of_Experience': 'int', 'Salary': 'float'}\n",
            "Dataset schema saved to 'dataset_schema.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# # Define the schema for each table in your dataset\n",
        "# # Replace the column names and data types with those from your dataset\n",
        "\n",
        "# # Employees table schema\n",
        "# employees_schema = {\n",
        "#     \"EmployeeID\": \"int\",\n",
        "#     \"FirstName\": \"str\",\n",
        "#     \"LastName\": \"str\",\n",
        "#     \"DepartmentID\": \"int\",\n",
        "#     \"Years_of_Experience\": \"int\",\n",
        "#     # Add more columns as needed\n",
        "# }\n",
        "\n",
        "# # Departments table schema\n",
        "# departments_schema = {\n",
        "#     \"DepartmentID\": \"int\",\n",
        "#     \"DepartmentName\": \"str\",\n",
        "#     # Add more columns as needed\n",
        "# }\n",
        "\n",
        "# # Salaries table schema\n",
        "# salaries_schema = {\n",
        "#     \"EmployeeID\": \"int\",\n",
        "#     \"Salary\": \"float\",\n",
        "#     # Add more columns as needed\n",
        "# }\n",
        "\n",
        "# # Combine schemas into a dictionary\n",
        "# schema_dict = {\n",
        "#     \"Employees\": employees_schema,\n",
        "#     \"Departments\": departments_schema,\n",
        "#     \"Salaries\": salaries_schema\n",
        "# }\n",
        "\n",
        "# # Write schema information to a JSON file\n",
        "# schema_file_path = \"schema.json\"\n",
        "# with open(schema_file_path, \"w\") as f:\n",
        "#     json.dump(schema_dict, f)\n",
        "\n",
        "# print(\"Schema information written to:\", schema_file_path)\n"
      ],
      "metadata": {
        "id": "N8pUmZEgRxPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# # Load the schema information from the JSON file\n",
        "# schema_file_path = \"schema.json\"  # Path to the schema JSON file\n",
        "# with open(schema_file_path, \"r\") as f:\n",
        "#     schema_dict = json.load(f)\n",
        "\n",
        "# # Example of accessing schema information\n",
        "# employees_schema = schema_dict[\"Employees\"]\n",
        "# departments_schema = schema_dict[\"Departments\"]\n",
        "# salaries_schema = schema_dict[\"Salaries\"]\n",
        "\n",
        "# # Example of using schema information to generate SQL queries\n",
        "# def generate_sql_query(user_prompt, schema_dict):\n",
        "#     # Logic to generate SQL query based on user prompt and schema\n",
        "#     # Example:\n",
        "#     # If user_prompt mentions a column name, check its data type in the schema\n",
        "#     # and construct the appropriate SQL query\n",
        "#     pass\n",
        "\n",
        "# # Example of generating SQL query using user prompt and schema\n",
        "# user_prompt = \"Generate a SQL query to select the age, gender, and salary of employees who are Software Engineers with at least 5 years of experience.\"\n",
        "# generated_query = generate_sql_query(user_prompt, schema_dict)\n",
        "# print(\"Generated SQL query:\", generated_query)\n"
      ],
      "metadata": {
        "id": "Zkhq8MbNU63k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from llama_cpp import Llama\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Read dataset\n",
        "dataset_path = \"/content/drive/MyDrive/new code/Salary Data.csv\"\n",
        "if not os.path.exists(dataset_path):\n",
        "    raise FileNotFoundError(f\"File '{dataset_path}' not found\")\n",
        "\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Load schema information from JSON file\n",
        "schema_file_path = \"dataset_schema.json\"\n",
        "with open(schema_file_path, \"r\") as f:\n",
        "    schema_dict = json.load(f)\n",
        "\n",
        "# Define SQL schema based on dataset variables\n",
        "schema = \"\"\"\n",
        "CREATE TABLE SalaryData (\n",
        "    Age INTEGER,\n",
        "    Gender TEXT,\n",
        "    Education_Level TEXT,\n",
        "    Job_Title TEXT,\n",
        "    Years_of_Experience INTEGER,\n",
        "    Salary FLOAT\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# User prompt\n",
        "user_prompt = \"Generate a SQL query to find Data Analyst Salaries from my dataset\"\n",
        "\n",
        "\n",
        "# Set path to the LLM model\n",
        "model_path = \"/content/drive/MyDrive/new code/Llama-2-FT.gguf\"\n",
        "absolute_model_path = os.path.abspath(model_path)\n",
        "\n",
        "# Initialize LLM\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    chat_format=\"chatml\"\n",
        ")\n",
        "\n",
        "# Generate response with schema information\n",
        "response = llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": f\"Generating SQL query based on user question and reffering to dataset schema: '{user_prompt}'.\"},\n",
        "        {\"role\": \"system\", \"content\": user_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "        {\"role\": \"system\", \"content\": schema_dict}  # Pass schema information\n",
        "    ],\n",
        "    response_format={\"type\": \"json_object\"},\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "# Extract and print the generated SQL query\n",
        "generated_query = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "print(\"Generated SQL query:\")\n",
        "print(generated_query)\n"
      ],
      "metadata": {
        "id": "lXrLTGlA6V3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9129b968-08ac-4521-e064-7561328e23ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /content/drive/MyDrive/new code/Llama-2-FT.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q8_0:  226 tensors\n",
            "llm_load_vocab: special tokens cache size = 259\n",
            "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  6828.64 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
            "from_string grammar:\n",
            "root ::= object \n",
            "object ::= [{] ws object_11 [}] ws \n",
            "value ::= object | array | string | number | value_6 ws \n",
            "array ::= [[] ws array_15 []] ws \n",
            "string ::= [\"] string_18 [\"] ws \n",
            "number ::= number_19 number_25 number_29 ws \n",
            "value_6 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
            "ws ::= ws_31 \n",
            "object_8 ::= string [:] ws value object_10 \n",
            "object_9 ::= [,] ws string [:] ws value \n",
            "object_10 ::= object_9 object_10 | \n",
            "object_11 ::= object_8 | \n",
            "array_12 ::= value array_14 \n",
            "array_13 ::= [,] ws value \n",
            "array_14 ::= array_13 array_14 | \n",
            "array_15 ::= array_12 | \n",
            "string_16 ::= [^\"\\<U+0000>-<U+001F>] | [\\] string_17 \n",
            "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
            "string_18 ::= string_16 string_18 | \n",
            "number_19 ::= number_20 number_21 \n",
            "number_20 ::= [-] | \n",
            "number_21 ::= [0-9] | [1-9] number_22 \n",
            "number_22 ::= [0-9] number_22 | \n",
            "number_23 ::= [.] number_24 \n",
            "number_24 ::= [0-9] number_24 | [0-9] \n",
            "number_25 ::= number_23 | \n",
            "number_26 ::= [eE] number_27 number_28 \n",
            "number_27 ::= [-+] | \n",
            "number_28 ::= [0-9] number_28 | [0-9] \n",
            "number_29 ::= number_26 | \n",
            "ws_30 ::= [ <U+0009><U+000A>] ws \n",
            "ws_31 ::= ws_30 | \n",
            "\n",
            "\n",
            "llama_print_timings:        load time =   41146.48 ms\n",
            "llama_print_timings:      sample time =     536.40 ms /    68 runs   (    7.89 ms per token,   126.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =   41145.75 ms /    93 tokens (  442.43 ms per token,     2.26 tokens per second)\n",
            "llama_print_timings:        eval time =   55956.51 ms /    67 runs   (  835.17 ms per token,     1.20 tokens per second)\n",
            "llama_print_timings:       total time =   97810.11 ms /   160 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated SQL query:\n",
            "{\n",
            "    \"SELECT\" : \"SELECT data_analyst_salary FROM table_1 WHERE data_analyst_salary > 50000 AND data_analyst_salary < 70000\",\n",
            "    \"TABLE_1\" : \"table_1\"\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tkPdTcw6IS-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DYK6XPs-cY8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automating the Process of Dataset Schema to a JSON file\n"
      ],
      "metadata": {
        "id": "KiKYJrfXcZF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from llama_cpp import Llama\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Read dataset\n",
        "dataset_path = \"/content/drive/MyDrive/new code/New_Salary Data.csv\"\n",
        "if not os.path.exists(dataset_path):\n",
        "    raise FileNotFoundError(f\"File '{dataset_path}' not found\")\n",
        "\n",
        "dataset = pd.read_csv(dataset_path)"
      ],
      "metadata": {
        "id": "0TZa-H4LIfL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea963a46-69d4-4d6b-8504-b62d5417a563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract schema information from the dataset\n",
        "schema_dict = {}\n",
        "for column_name, dtype in dataset.dtypes.items():\n",
        "    schema_dict[column_name] = dtype.name\n",
        "\n",
        "# Print the inferred schema\n",
        "print(\"Inferred schema:\")\n",
        "print(schema_dict)\n",
        "\n",
        "# Save the schema to a JSON file\n",
        "with open('dataset_schema.json', 'w') as f:\n",
        "    json.dump(schema_dict, f)\n",
        "\n",
        "print(\"Dataset schema saved to 'dataset_schema.json'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OjVgxDPcg16",
        "outputId": "dd2a24b2-5c32-4457-f00b-bf6dd6c64f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inferred schema:\n",
            "{'Age': 'float64', 'Gender': 'object', 'Education Level': 'object', 'Job Title': 'object', 'Years of Experience': 'float64', 'Salary': 'float64', 'EmpID': 'int64', 'FirstName': 'object', 'LastName': 'object'}\n",
            "Dataset schema saved to 'dataset_schema.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define SQL schema based on dataset variables\n",
        "table_name = os.path.splitext(os.path.basename(dataset_path))[0]\n",
        "columns = \",\\n\".join([f\"{column} {dtype.upper()}\" for column, dtype in schema_dict.items()])\n",
        "schema = f\"\"\"\n",
        "CREATE TABLE {table_name} (\n",
        "    {columns}\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# User prompt\n",
        "user_prompt = \"Generate a SQL query to find employees with years of experience between 1 and 5 \"\n",
        "\n",
        "# Set path to the LLM model\n",
        "model_path = \"/content/drive/MyDrive/new code/Llama-2-FT.gguf\"\n",
        "absolute_model_path = os.path.abspath(model_path)\n",
        "\n",
        "# Initialize LLM\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    chat_format=\"chatml\"\n",
        ")\n",
        "\n",
        "# # Generate response with schema information\n",
        "# response = llm.create_chat_completion(\n",
        "#     messages=[\n",
        "#         {\"role\": \"system\", \"content\": f\"Generating SQL query based on user question and referring to inferred dataset schema: '{user_prompt}'.\"},\n",
        "#         {\"role\": \"system\", \"content\": user_prompt},\n",
        "#         {\"role\": \"user\", \"content\": user_prompt},\n",
        "#         {\"role\": \"system\", \"content\": schema_dict}  # Pass inferred schema information\n",
        "#     ],\n",
        "#     response_format={\"type\": \"json_object\"},\n",
        "#     temperature=0.1,\n",
        "# )\n",
        "\n",
        "# Generate response with schema information\n",
        "response = llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": f\"Generating SQL query based on the user's question and inferred dataset schema: '{user_prompt}'.\"},\n",
        "        {\"role\": \"system\", \"content\": f\"User query: '{user_prompt}'\"},\n",
        "        {\"role\": \"system\", \"content\": \"Using inferred dataset schema to formulate the SQL query:\"},\n",
        "        {\"role\": \"system\", \"content\": schema_dict}  # Pass inferred schema information\n",
        "    ],\n",
        "    response_format={\"type\": \"json_object\"},\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "# Extract and print the generated SQL query\n",
        "generated_query = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "print(\"Generated SQL query:\")\n",
        "print(generated_query)\n"
      ],
      "metadata": {
        "id": "VQ8pPQ6R6Ux7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a1df77c-30ab-4e28-ff32-82da50f8077e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /content/drive/MyDrive/new code/Llama-2-FT.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q8_0:  226 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  6828.64 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
            "from_string grammar:\n",
            "root ::= object \n",
            "object ::= [{] ws object_11 [}] ws \n",
            "value ::= object | array | string | number | value_6 ws \n",
            "array ::= [[] ws array_15 []] ws \n",
            "string ::= [\"] string_18 [\"] ws \n",
            "number ::= number_19 number_25 number_29 ws \n",
            "value_6 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
            "ws ::= ws_31 \n",
            "object_8 ::= string [:] ws value object_10 \n",
            "object_9 ::= [,] ws string [:] ws value \n",
            "object_10 ::= object_9 object_10 | \n",
            "object_11 ::= object_8 | \n",
            "array_12 ::= value array_14 \n",
            "array_13 ::= [,] ws value \n",
            "array_14 ::= array_13 array_14 | \n",
            "array_15 ::= array_12 | \n",
            "string_16 ::= [^\"\\<U+0000>-<U+001F>] | [\\] string_17 \n",
            "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
            "string_18 ::= string_16 string_18 | \n",
            "number_19 ::= number_20 number_21 \n",
            "number_20 ::= [-] | \n",
            "number_21 ::= [0-9] | [1-9] number_22 \n",
            "number_22 ::= [0-9] number_22 | \n",
            "number_23 ::= [.] number_24 \n",
            "number_24 ::= [0-9] number_24 | [0-9] \n",
            "number_25 ::= number_23 | \n",
            "number_26 ::= [eE] number_27 number_28 \n",
            "number_27 ::= [-+] | \n",
            "number_28 ::= [0-9] number_28 | [0-9] \n",
            "number_29 ::= number_26 | \n",
            "ws_30 ::= [ <U+0009><U+000A>] ws \n",
            "ws_31 ::= ws_30 | \n",
            "\n",
            "\n",
            "llama_print_timings:        load time =   26001.14 ms\n",
            "llama_print_timings:      sample time =     538.95 ms /    37 runs   (   14.57 ms per token,    68.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =   25931.18 ms /    65 tokens (  398.94 ms per token,     2.51 tokens per second)\n",
            "llama_print_timings:        eval time =   30118.43 ms /    36 runs   (  836.62 ms per token,     1.20 tokens per second)\n",
            "llama_print_timings:       total time =   56757.05 ms /   101 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated SQL query:\n",
            "{\n",
            "    \"query\": \"SELECT COUNT(*) FROM table_name WHERE years_of_experience BETWEEN 1 AND 5\"\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract and print the generated SQL query\n",
        "generated_query = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "print(\"Generated SQL query:\")\n",
        "print(generated_query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPCzQtz6r0wr",
        "outputId": "58b11996-e74b-44c6-b910-c5f406ca07e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated SQL query:\n",
            "{\n",
            "    \"query\": \"SELECT COUNT(*) FROM table_name WHERE years_of_experience BETWEEN 1 AND 5\"\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQgM4Dcyt36-",
        "outputId": "631bffce-2007-4caa-98e1-ac087f555937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.1-py3-none-any.whl (973 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_core-0.2.3-py3-none-any.whl (310 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.65-py3-none-any.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Installing collected packages: packaging, orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.1 langchain-core-0.2.3 langchain-text-splitters-0.2.0 langsmith-0.1.65 orjson-3.10.3 packaging-23.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure you have the required packages\n",
        "!pip install langchain==0.1.17 llama-cpp-python pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns48kR50lWJm",
        "outputId": "87454c10-077b-4e01-e998-a507f7ebdcae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.1.17\n",
            "  Downloading langchain-0.1.17-py3-none-any.whl (867 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.76)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.17) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.17) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.17) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.17) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.17)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.17) (1.33)\n",
            "Collecting langchain-community<0.1,>=0.0.36 (from langchain==0.1.17)\n",
            "  Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.48 (from langchain==0.1.17)\n",
            "  Downloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.17)\n",
            "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.17) (0.1.65)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.17) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.17) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.17) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.17) (8.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.11.0)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.17) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.17) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.17) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.17) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.17) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.17)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.17)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.17) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.48->langchain==0.1.17) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.17) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.17) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.17) (2.18.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.17) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.17) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.17) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.17) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.17) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.17)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.2.3\n",
            "    Uninstalling langchain-core-0.2.3:\n",
            "      Successfully uninstalled langchain-core-0.2.3\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.2.0\n",
            "    Uninstalling langchain-text-splitters-0.2.0:\n",
            "      Successfully uninstalled langchain-text-splitters-0.2.0\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.2.1\n",
            "    Uninstalling langchain-0.2.1:\n",
            "      Successfully uninstalled langchain-0.2.1\n",
            "Successfully installed dataclasses-json-0.6.6 langchain-0.1.17 langchain-community-0.0.38 langchain-core-0.1.52 langchain-text-splitters-0.0.2 marshmallow-3.21.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"./index.md\")\n",
        "loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "acY6eq85uSbx",
        "outputId": "a5e7a540-9702-4b88-f300-156c8298773a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error loading ./index.md",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/text.py\u001b[0m in \u001b[0;36mlazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './index.md'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-436bf391163e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./index.md\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/document_loaders/base.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;34m\"\"\"Load data into Document objects.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/text.py\u001b[0m in \u001b[0;36mlazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error loading {self.file_path}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error loading {self.file_path}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error loading ./index.md"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from langchain.agents import create_sql_agent\n",
        "\n",
        "# Now your Google Drive is mounted, you can access files from it\n",
        "model_path = \"/content/drive/MyDrive/new code/Llama-2-FT.gguf\"\n",
        "\n",
        "# Assuming you have already instantiated your LLM model\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    chat_format=\"chatml\"\n",
        ")\n",
        "\n",
        "# Create the SQL agent\n",
        "agent = create_sql_agent(\n",
        "    llm=llm,  # Your instantiated LLM model\n",
        "    agent_type=\"ZERO_SHOT_REACT_DESCRIPTION\",  # Agent type\n",
        "    toolkit=None,  # Toolkit for the agent (you can set it to None if not needed)\n",
        "    db=None,  # Database connection (you can set it to None if not needed)\n",
        "    verbose=True,  # Whether to enable verbose mode\n",
        ")\n",
        "\n",
        "# Define a function to interact with the SQL agent\n",
        "def generate_sql_query(prompt):\n",
        "    # Generate SQL query using the agent\n",
        "    sql_query = agent.run(prompt)\n",
        "    return sql_query\n",
        "\n",
        "# Example user prompt\n",
        "user_prompt = \"Generate a SQL query to find employees with years of experience between 1 and 5\"\n",
        "\n",
        "# Generate SQL query based on user prompt\n",
        "generated_query = generate_sql_query(user_prompt)\n",
        "\n",
        "# Use the generated SQL query in your application logic\n",
        "print(\"Generated SQL query:\", generated_query)\n",
        "# Execute the SQL query, display it to the user, or further process it as needed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rMdEP6LtR2e",
        "outputId": "55f218f5-87b3-41a5-e9f7-b25347ae212c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /content/drive/MyDrive/new code/Llama-2-FT.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q8_0:  226 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  6828.64 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Must provide exactly one of 'toolkit' or 'db'. Received neither.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-ab520cb54702>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Create the SQL agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m agent = create_sql_agent(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Your instantiated LLM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0magent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ZERO_SHOT_REACT_DESCRIPTION\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Agent type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/agent_toolkits/sql/base.py\u001b[0m in \u001b[0;36mcreate_sql_agent\u001b[0;34m(llm, toolkit, agent_type, callback_manager, prefix, suffix, format_instructions, input_variables, top_k, max_iterations, max_execution_time, early_stopping_method, verbose, agent_executor_kwargs, extra_tools, db, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtoolkit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0;34m\"Must provide exactly one of 'toolkit' or 'db'. Received neither.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Must provide exactly one of 'toolkit' or 'db'. Received neither."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detailed Schema Documentation and storing Metadata"
      ],
      "metadata": {
        "id": "MJtWiH0Ly39o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_cpp import Llama\n",
        "# import pprint\n",
        "# from google.colab import drive\n",
        "# import os\n",
        "# import pandas as pd\n",
        "# import json\n",
        "\n",
        "# # Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Set paths\n",
        "# model_path = \"/content/drive/MyDrive/Final Working LLM Code/Llama-2-FT.gguf\"\n",
        "# dataset_path = \"/content/drive/MyDrive/Final Working LLM Code/New_Salary Data.csv\"\n",
        "# schema_file_path = \"dataset_schema.json\"\n",
        "\n",
        "# # Load the dataset from CSV file\n",
        "# df = pd.read_csv(dataset_path)\n",
        "\n",
        "# # Define the schema with detailed descriptions\n",
        "# schema = {\n",
        "#     \"tables\": {\n",
        "#         \"SalaryData\": {\n",
        "#             \"description\": \"Table containing employee salary data\",\n",
        "#             \"columns\": {\n",
        "#                 \"Age\": {\n",
        "#                     \"type\": \"integer\",\n",
        "#                     \"description\": \"Age of the employee in years\"\n",
        "#                 },\n",
        "#                 \"Gender\": {\n",
        "#                     \"type\": \"string\",\n",
        "#                     \"description\": \"Gender of the employee\"\n",
        "#                 },\n",
        "#                 \"Education_Level\": {\n",
        "#                     \"type\": \"string\",\n",
        "#                     \"description\": \"Highest education level attained by the employee\"\n",
        "#                 },\n",
        "#                 \"Job_Title\": {\n",
        "#                     \"type\": \"string\",\n",
        "#                     \"description\": \"Job title of the employee\"\n",
        "#                 },\n",
        "#                 \"Years_of_Experience\": {\n",
        "#                     \"type\": \"integer\",\n",
        "#                     \"description\": \"Number of years the employee has worked\"\n",
        "#                 },\n",
        "#                 \"Salary\": {\n",
        "#                     \"type\": \"float\",\n",
        "#                     \"description\": \"Annual salary of the employee\"\n",
        "#                 },\n",
        "#                 \"EmpID\": {\n",
        "#                     \"type\": \"integer\",\n",
        "#                     \"description\": \"Unique identifier for the employee\"\n",
        "#                 },\n",
        "#                 \"FirstName\": {\n",
        "#                     \"type\": \"string\",\n",
        "#                     \"description\": \"First name of the employee\"\n",
        "#                 },\n",
        "#                 \"LastName\": {\n",
        "#                     \"type\": \"string\",\n",
        "#                     \"description\": \"Last name of the employee\"\n",
        "#                 }\n",
        "#             }\n",
        "#         }\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# # Print the defined schema\n",
        "# print(\"New_SalaryData table schema:\")\n",
        "# print(json.dumps(schema, indent=4))\n",
        "\n",
        "# # Save the schema to a JSON file\n",
        "# with open(schema_file_path, 'w') as f:\n",
        "#     json.dump(schema, f)\n",
        "\n",
        "# print(\"Dataset schema saved to 'dataset_schema.json'\")\n",
        "\n",
        "# # Load schema information from JSON file\n",
        "# with open(schema_file_path, \"r\") as f:\n",
        "#     schema_dict = json.load(f)\n",
        "\n",
        "# # Define SQL schema based on dataset variables\n",
        "# sql_schema = \"\"\"\n",
        "# CREATE TABLE SalaryData (\n",
        "#     Age INTEGER,\n",
        "#     Gender TEXT,\n",
        "#     Education_Level TEXT,\n",
        "#     Job_Title TEXT,\n",
        "#     Years_of_Experience INTEGER,\n",
        "#     Salary FLOAT,\n",
        "#     EmpID INTEGER,\n",
        "#     FirstName TEXT,\n",
        "#     LastName TEXT\n",
        "# )\n",
        "# \"\"\"\n",
        "\n",
        "# # User prompt\n",
        "# user_prompt = \"Generate a SQL query to find List of the employees with a salary greater than $100,000.\"\n",
        "\n",
        "# # Initialize LLM\n",
        "# llm = Llama(\n",
        "#     model_path=model_path,\n",
        "#     chat_format=\"chatml\"\n",
        "# )\n",
        "\n",
        "# # Generate response with schema information\n",
        "# response = llm.create_chat_completion(\n",
        "#     messages=[\n",
        "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant trained to generate SQL queries based on given database schema and user prompts.\"},\n",
        "#         {\"role\": \"system\", \"content\": f\"Schema: {json.dumps(schema)}\"},\n",
        "#         {\"role\": \"user\", \"content\": user_prompt},\n",
        "#         {\"role\": \"assistant\", \"content\": \"Please provide the SQL query based on the provided schema and prompt.\"}\n",
        "#     ],\n",
        "#     response_format={\"type\": \"text\"},\n",
        "#     temperature=0.1,\n",
        "# )\n",
        "\n",
        "# # Extract and print the generated SQL query\n",
        "# generated_query = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "# print(\"Generated SQL query:\")\n",
        "# print(generated_query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-7GBmVPYAOj",
        "outputId": "589f75cb-b593-4f7f-8cd1-d10194193ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "New_SalaryData table schema:\n",
            "{\n",
            "    \"tables\": {\n",
            "        \"SalaryData\": {\n",
            "            \"description\": \"Table containing employee salary data\",\n",
            "            \"columns\": {\n",
            "                \"Age\": {\n",
            "                    \"type\": \"integer\",\n",
            "                    \"description\": \"Age of the employee in years\"\n",
            "                },\n",
            "                \"Gender\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"Gender of the employee\"\n",
            "                },\n",
            "                \"Education_Level\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"Highest education level attained by the employee\"\n",
            "                },\n",
            "                \"Job_Title\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"Job title of the employee\"\n",
            "                },\n",
            "                \"Years_of_Experience\": {\n",
            "                    \"type\": \"integer\",\n",
            "                    \"description\": \"Number of years the employee has worked\"\n",
            "                },\n",
            "                \"Salary\": {\n",
            "                    \"type\": \"float\",\n",
            "                    \"description\": \"Annual salary of the employee\"\n",
            "                },\n",
            "                \"EmpID\": {\n",
            "                    \"type\": \"integer\",\n",
            "                    \"description\": \"Unique identifier for the employee\"\n",
            "                },\n",
            "                \"FirstName\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"First name of the employee\"\n",
            "                },\n",
            "                \"LastName\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"Last name of the employee\"\n",
            "                }\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "}\n",
            "Dataset schema saved to 'dataset_schema.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /content/drive/MyDrive/Final Working LLM Code/Llama-2-FT.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q8_0:  226 tensors\n",
            "llm_load_vocab: special tokens cache size = 259\n",
            "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  6828.64 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
            "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
            "\n",
            "llama_print_timings:        load time =   47662.20 ms\n",
            "llama_print_timings:      sample time =      12.67 ms /    22 runs   (    0.58 ms per token,  1736.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =   47657.89 ms /   121 tokens (  393.87 ms per token,     2.54 tokens per second)\n",
            "llama_print_timings:        eval time =   17128.76 ms /    21 runs   (  815.66 ms per token,     1.23 tokens per second)\n",
            "llama_print_timings:       total time =   64819.84 ms /   142 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated SQL query:\n",
            "SELECT employee FROM employees WHERE salary > 100000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_cpp import Llama\n",
        "# import pprint\n",
        "# from google.colab import drive\n",
        "# import os\n",
        "# import pandas as pd\n",
        "# import json\n",
        "\n",
        "# # Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Set paths\n",
        "# model_path = \"/content/drive/MyDrive/Final Working LLM Code/Llama-2-FT.gguf\"\n",
        "# dataset_path = \"/content/drive/MyDrive/Final Working LLM Code/New_Salary Data.csv\"\n",
        "# schema_file_path = \"dataset_schema.json\"\n",
        "\n",
        "# # Load the dataset from CSV file\n",
        "# df = pd.read_csv(dataset_path)\n",
        "\n",
        "# # Define the schema with detailed descriptions\n",
        "# schema = {\n",
        "#     \"tables\": {\n",
        "#         \"SalaryData\": {\n",
        "#             \"description\": \"Table containing employee salary data\",\n",
        "#             \"columns\": {\n",
        "#                 \"Age\": {\n",
        "#                     \"type\": \"integer\",\n",
        "#                     \"description\": \"Age of the employee in years\"\n",
        "#                 },\n",
        "#                 \"Gender\": {\n",
        "#                     \"type\": \"string\",\n",
        "#                     \"description\": \"Gender of the employee\"\n",
        "#                 },\n",
        "#                 \"Education_Level\": {\n",
        "#                     \"type\": \"string\",\n",
        "#                     \"description\": \"Highest education level attained by the employee\"\n",
        "#                 },\n",
        "#                 \"Job_Title\": {\n",
        "#                     \"type\": \"string\",\n",
        "#                     \"description\": \"Job title of the employee\"\n",
        "#                 },\n",
        "#                 \"Years_of_Experience\": {\n",
        "#                     \"type\": \"integer\",\n",
        "#                     \"description\": \"Number of years the employee has worked\"\n",
        "#                 },\n",
        "#                 \"Salary\": {\n",
        "#                     \"type\": \"float\",\n",
        "#                     \"description\": \"Annual salary of the employee\"\n",
        "#                 },\n",
        "#                 \"EmpID\": {\n",
        "#                     \"type\": \"integer\",\n",
        "#                     \"description\": \"Unique identifier for the employee\"\n",
        "#                 },\n",
        "#                 \"FirstName\": {\n",
        "#                     \"type\": \"string\",\n",
        "#                     \"description\": \"First name of the employee\"\n",
        "#                 },\n",
        "#                 \"LastName\": {\n",
        "#                     \"type\": \"string\",\n",
        "#                     \"description\": \"Last name of the employee\"\n",
        "#                 }\n",
        "#             }\n",
        "#         }\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# # Print the defined schema\n",
        "# print(\"SalaryData table schema:\")\n",
        "# print(json.dumps(schema, indent=4))\n",
        "\n",
        "# # Save the schema to a JSON file\n",
        "# with open(schema_file_path, 'w') as f:\n",
        "#     json.dump(schema, f)\n",
        "\n",
        "# print(\"Dataset schema saved to 'dataset_schema.json'\")\n",
        "\n",
        "# # Load schema information from JSON file\n",
        "# with open(schema_file_path, \"r\") as f:\n",
        "#     schema_dict = json.load(f)\n",
        "\n",
        "# # Define SQL schema based on dataset variables\n",
        "# sql_schema = \"\"\"\n",
        "# CREATE TABLE SalaryData (\n",
        "#     Age INTEGER,\n",
        "#     Gender TEXT,\n",
        "#     Education_Level TEXT,\n",
        "#     Job_Title TEXT,\n",
        "#     Years_of_Experience INTEGER,\n",
        "#     Salary FLOAT,\n",
        "#     EmpID INTEGER,\n",
        "#     FirstName TEXT,\n",
        "#     LastName TEXT\n",
        "# )\n",
        "# \"\"\"\n",
        "\n",
        "# # User prompt\n",
        "# user_prompt = \"Generate a SQL query to list the employees with job title 'Software Engineer' with salary greater than $100,000 from Salary and Job Title\"\n",
        "\n",
        "# # Initialize LLM\n",
        "# llm = Llama(\n",
        "#     model_path=model_path,\n",
        "#     chat_format=\"chatml\"\n",
        "# )\n",
        "\n",
        "# # Generate response with schema information\n",
        "# response = llm.create_chat_completion(\n",
        "#     messages=[\n",
        "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant trained to generate SQL queries based on the given database schema and user prompts.\"},\n",
        "#         {\"role\": \"system\", \"content\": f\"Schema: {json.dumps(schema)}\"},\n",
        "#         {\"role\": \"user\", \"content\": user_prompt},\n",
        "#         {\"role\": \"assistant\", \"content\": \"Please provide the SQL query based on the provided schema and prompt, using the table name and the correct column names from the schema.\"}\n",
        "#     ],\n",
        "#     response_format={\"type\": \"text\"},\n",
        "#     temperature=0.1,\n",
        "# )\n",
        "\n",
        "# # Extract and print the generated SQL query\n",
        "# generated_query = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "# print(\"Generated SQL query:\")\n",
        "# print(generated_query)\n"
      ],
      "metadata": {
        "id": "D-QY-a9TdkSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "672bf4f6-c469-4111-92b6-dad81c77dfae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "SalaryData table schema:\n",
            "{\n",
            "    \"tables\": {\n",
            "        \"SalaryData\": {\n",
            "            \"description\": \"Table containing employee salary data\",\n",
            "            \"columns\": {\n",
            "                \"Age\": {\n",
            "                    \"type\": \"integer\",\n",
            "                    \"description\": \"Age of the employee in years\"\n",
            "                },\n",
            "                \"Gender\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"Gender of the employee\"\n",
            "                },\n",
            "                \"Education_Level\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"Highest education level attained by the employee\"\n",
            "                },\n",
            "                \"Job_Title\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"Job title of the employee\"\n",
            "                },\n",
            "                \"Years_of_Experience\": {\n",
            "                    \"type\": \"integer\",\n",
            "                    \"description\": \"Number of years the employee has worked\"\n",
            "                },\n",
            "                \"Salary\": {\n",
            "                    \"type\": \"float\",\n",
            "                    \"description\": \"Annual salary of the employee\"\n",
            "                },\n",
            "                \"EmpID\": {\n",
            "                    \"type\": \"integer\",\n",
            "                    \"description\": \"Unique identifier for the employee\"\n",
            "                },\n",
            "                \"FirstName\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"First name of the employee\"\n",
            "                },\n",
            "                \"LastName\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"Last name of the employee\"\n",
            "                }\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "}\n",
            "Dataset schema saved to 'dataset_schema.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /content/drive/MyDrive/Final Working LLM Code/Llama-2-FT.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q8_0:  226 tensors\n",
            "llm_load_vocab: special tokens cache size = 259\n",
            "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  6828.64 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
            "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
            "\n",
            "llama_print_timings:        load time =   58226.02 ms\n",
            "llama_print_timings:      sample time =      17.99 ms /    28 runs   (    0.64 ms per token,  1556.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =   58225.65 ms /   147 tokens (  396.09 ms per token,     2.52 tokens per second)\n",
            "llama_print_timings:        eval time =   22193.54 ms /    27 runs   (  821.98 ms per token,     1.22 tokens per second)\n",
            "llama_print_timings:       total time =   80462.04 ms /   174 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated SQL query:\n",
            "SELECT job_title FROM salary WHERE salary > 100000 AND job_title = \"software engineer\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "import pprint\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths\n",
        "model_path = \"/content/drive/MyDrive/Final Working LLM Code/Llama-2-FT.gguf\"\n",
        "dataset_path = \"/content/drive/MyDrive/Final Working LLM Code/New_Salary Data.csv\"\n",
        "schema_file_path = \"dataset_schema.json\"\n"
      ],
      "metadata": {
        "id": "OxW_VFid7NXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f084f28-36ed-464a-acc8-9eb085102a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the dataset from CSV file\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Define the schema with detailed descriptions\n",
        "schema = {\n",
        "    \"tables\": {\n",
        "        \"New_SalaryData\": {\n",
        "            \"description\": \"Table containing employee salary data\",\n",
        "            \"columns\": {\n",
        "                \"Age\": {\n",
        "                    \"type\": \"integer\",\n",
        "                    \"description\": \"Age of the employee in years\"\n",
        "                },\n",
        "                \"Gender\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Gender of the employee\"\n",
        "                },\n",
        "                \"Education_Level\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Highest education level attained by the employee\"\n",
        "                },\n",
        "                \"Job_Title\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Job title of the employee\"\n",
        "                },\n",
        "                \"Years_of_Experience\": {\n",
        "                    \"type\": \"integer\",\n",
        "                    \"description\": \"Number of years the employee has worked\"\n",
        "                },\n",
        "                \"Salary\": {\n",
        "                    \"type\": \"float\",\n",
        "                    \"description\": \"Annual salary of the employee\"\n",
        "                },\n",
        "                \"EmpID\": {\n",
        "                    \"type\": \"integer\",\n",
        "                    \"description\": \"Unique identifier for the employee\"\n",
        "                },\n",
        "                \"FirstName\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"First name of the employee\"\n",
        "                },\n",
        "                \"LastName\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Last name of the employee\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Print the defined schema\n",
        "print(\"SalaryData table schema:\")\n",
        "print(json.dumps(schema, indent=4))\n",
        "\n",
        "# Save the schema to a JSON file\n",
        "with open(schema_file_path, 'w') as f:\n",
        "    json.dump(schema, f)\n",
        "\n",
        "print(\"Dataset schema saved to 'dataset_schema.json'\")\n",
        "\n",
        "# Load schema information from JSON file\n",
        "with open(schema_file_path, \"r\") as f:\n",
        "    schema_dict = json.load(f)\n",
        "\n",
        "# Define SQL schema based on dataset variables\n",
        "sql_schema = \"\"\"\n",
        "CREATE TABLE SalaryData (\n",
        "    Age INTEGER,\n",
        "    Gender TEXT,\n",
        "    Education_Level TEXT,\n",
        "    Job_Title TEXT,\n",
        "    Years_of_Experience INTEGER,\n",
        "    Salary FLOAT,\n",
        "    EmpID INTEGER,\n",
        "    FirstName TEXT,\n",
        "    LastName TEXT\n",
        ")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZh4u6z_qMpS",
        "outputId": "922bf1a8-e841-43cf-d1d6-476c3a60332c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SalaryData table schema:\n",
            "{\n",
            "    \"tables\": {\n",
            "        \"New_SalaryData\": {\n",
            "            \"description\": \"Table containing employee salary data\",\n",
            "            \"columns\": {\n",
            "                \"Age\": {\n",
            "                    \"type\": \"integer\",\n",
            "                    \"description\": \"Age of the employee in years\"\n",
            "                },\n",
            "                \"Gender\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"Gender of the employee\"\n",
            "                },\n",
            "                \"Education_Level\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"Highest education level attained by the employee\"\n",
            "                },\n",
            "                \"Job_Title\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"Job title of the employee\"\n",
            "                },\n",
            "                \"Years_of_Experience\": {\n",
            "                    \"type\": \"integer\",\n",
            "                    \"description\": \"Number of years the employee has worked\"\n",
            "                },\n",
            "                \"Salary\": {\n",
            "                    \"type\": \"float\",\n",
            "                    \"description\": \"Annual salary of the employee\"\n",
            "                },\n",
            "                \"EmpID\": {\n",
            "                    \"type\": \"integer\",\n",
            "                    \"description\": \"Unique identifier for the employee\"\n",
            "                },\n",
            "                \"FirstName\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"First name of the employee\"\n",
            "                },\n",
            "                \"LastName\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"Last name of the employee\"\n",
            "                }\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "}\n",
            "Dataset schema saved to 'dataset_schema.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User prompt\n",
        "user_prompt = \"Generate a SQL query to list the employees with starting letter U in their FirstName from the New_SalaryData table.\"\n",
        "\n",
        "# Initialize LLM\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    chat_format=\"chatml\"\n",
        ")\n",
        "\n",
        "# Generate response with schema information\n",
        "response = llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant trained to generate SQL queries based on the given database schema and user prompts.\"},\n",
        "        {\"role\": \"system\", \"content\": f\"Schema: {json.dumps(schema)}\"},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "        {\"role\": \"assistant\", \"content\": \"Please provide the SQL query based on the provided schema and prompt, using the table name and the correct column names from the schema.\"}\n",
        "    ],\n",
        "    response_format={\"type\": \"text\"},\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "# Extract and print the generated SQL query\n",
        "generated_query = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "print(\"Generated SQL query:\")\n",
        "print(generated_query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xHcp_Ltv3Hs",
        "outputId": "3197af39-ce91-425d-ccf1-785c2f197339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /content/drive/MyDrive/Final Working LLM Code/Llama-2-FT.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q8_0:  226 tensors\n",
            "llm_load_vocab: special tokens cache size = 259\n",
            "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  6828.64 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
            "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
            "\n",
            "llama_print_timings:        load time =   53442.48 ms\n",
            "llama_print_timings:      sample time =      11.83 ms /    18 runs   (    0.66 ms per token,  1521.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =   53442.22 ms /   135 tokens (  395.87 ms per token,     2.53 tokens per second)\n",
            "llama_print_timings:        eval time =   14401.25 ms /    17 runs   (  847.13 ms per token,     1.18 tokens per second)\n",
            "llama_print_timings:       total time =   67872.16 ms /   152 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated SQL query:\n",
            "SELECT FirstName FROM New_SalaryData WHERE StartingLetter = \"U\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "import pprint\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths\n",
        "model_path = \"/content/drive/MyDrive/Final Working LLM Code/Llama-2-FT.gguf\"\n",
        "dataset_path = \"/content/drive/MyDrive/Final Working LLM Code/New_Salary Data.csv\"\n",
        "schema_file_path = \"dataset_schema.json\"\n",
        "\n",
        "# Load the dataset from CSV file\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Define the schema with detailed descriptions\n",
        "schema = {\n",
        "    \"tables\": {\n",
        "        \"New_SalaryData\": {\n",
        "            \"description\": \"Table containing employee salary data\",\n",
        "            \"columns\": {\n",
        "                \"Age\": {\n",
        "                    \"type\": \"integer\",\n",
        "                    \"description\": \"Age of the employee in years\"\n",
        "                },\n",
        "                \"Gender\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Gender of the employee\"\n",
        "                },\n",
        "                \"Education_Level\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Highest education level attained by the employee\"\n",
        "                },\n",
        "                \"Job_Title\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Job title of the employee\"\n",
        "                },\n",
        "                \"Years_of_Experience\": {\n",
        "                    \"type\": \"integer\",\n",
        "                    \"description\": \"Number of years the employee has worked\"\n",
        "                },\n",
        "                \"Salary\": {\n",
        "                    \"type\": \"float\",\n",
        "                    \"description\": \"Annual salary of the employee\"\n",
        "                },\n",
        "                \"EmpID\": {\n",
        "                    \"type\": \"integer\",\n",
        "                    \"description\": \"Unique identifier for the employee\"\n",
        "                },\n",
        "                \"FirstName\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"First name of the employee\"\n",
        "                },\n",
        "                \"LastName\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Last name of the employee\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Print the defined schema\n",
        "print(\"New_SalaryData table schema:\")\n",
        "print(json.dumps(schema, indent=4))\n",
        "\n",
        "# Save the schema to a JSON file\n",
        "with open(schema_file_path, 'w') as f:\n",
        "    json.dump(schema, f)\n",
        "\n",
        "print(\"Dataset schema saved to 'dataset_schema.json'\")\n",
        "\n",
        "# Load schema information from JSON file\n",
        "with open(schema_file_path, \"r\") as f:\n",
        "    schema_dict = json.load(f)\n",
        "\n",
        "# Define SQL schema based on dataset variables\n",
        "sql_schema = \"\"\"\n",
        "CREATE TABLE New_SalaryData (\n",
        "    Age INTEGER,\n",
        "    Gender TEXT,\n",
        "    Education_Level TEXT,\n",
        "    Job_Title TEXT,\n",
        "    Years_of_Experience INTEGER,\n",
        "    Salary FLOAT,\n",
        "    EmpID INTEGER,\n",
        "    FirstName TEXT,\n",
        "    LastName TEXT\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# User prompt with more detailed instructions\n",
        "user_prompt = \"Generate a SQL query to Find all female with education level PhD\"\n",
        "# Initialize LLM\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    chat_format=\"chatml\"\n",
        ")\n",
        "\n",
        "# Generate response with schema information\n",
        "response = llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant trained to generate SQL queries based on the given database schema and user prompts.\"},\n",
        "        {\"role\": \"system\", \"content\": f\"Schema: {json.dumps(schema)}\"},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "        {\"role\": \"assistant\", \"content\": \"Please provide the SQL query based on the provided schema and prompt, using the table name and the correct column names from the schema.\"}\n",
        "    ],\n",
        "    response_format={\"type\": \"text\"},\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "# Extract and print the generated SQL query\n",
        "generated_query = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "print(\"Generated SQL query:\")\n",
        "print(generated_query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5P9dKPgwH_7",
        "outputId": "f7662eba-7fec-440b-c2aa-80417d93bc0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "New_SalaryData table schema:\n",
            "{\n",
            "    \"tables\": {\n",
            "        \"New_SalaryData\": {\n",
            "            \"description\": \"Table containing employee salary data\",\n",
            "            \"columns\": {\n",
            "                \"Age\": {\n",
            "                    \"type\": \"integer\",\n",
            "                    \"description\": \"Age of the employee in years\"\n",
            "                },\n",
            "                \"Gender\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"Gender of the employee\"\n",
            "                },\n",
            "                \"Education_Level\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"Highest education level attained by the employee\"\n",
            "                },\n",
            "                \"Job_Title\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"Job title of the employee\"\n",
            "                },\n",
            "                \"Years_of_Experience\": {\n",
            "                    \"type\": \"integer\",\n",
            "                    \"description\": \"Number of years the employee has worked\"\n",
            "                },\n",
            "                \"Salary\": {\n",
            "                    \"type\": \"float\",\n",
            "                    \"description\": \"Annual salary of the employee\"\n",
            "                },\n",
            "                \"EmpID\": {\n",
            "                    \"type\": \"integer\",\n",
            "                    \"description\": \"Unique identifier for the employee\"\n",
            "                },\n",
            "                \"FirstName\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"First name of the employee\"\n",
            "                },\n",
            "                \"LastName\": {\n",
            "                    \"type\": \"string\",\n",
            "                    \"description\": \"Last name of the employee\"\n",
            "                }\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "}\n",
            "Dataset schema saved to 'dataset_schema.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /content/drive/MyDrive/Final Working LLM Code/Llama-2-FT.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q8_0:  226 tensors\n",
            "llm_load_vocab: special tokens cache size = 259\n",
            "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  6828.64 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
            "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
            "\n",
            "llama_print_timings:        load time =   47840.74 ms\n",
            "llama_print_timings:      sample time =      16.44 ms /    25 runs   (    0.66 ms per token,  1520.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =   47840.53 ms /   124 tokens (  385.81 ms per token,     2.59 tokens per second)\n",
            "llama_print_timings:        eval time =   20351.05 ms /    24 runs   (  847.96 ms per token,     1.18 tokens per second)\n",
            "llama_print_timings:       total time =   68228.56 ms /   148 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated SQL query:\n",
            "SELECT COUNT(*) FROM table_name WHERE education_level = \"phd\" AND gender = \"female\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XxkuNA7QzJ41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using 2 different Tables and storing as Metadata"
      ],
      "metadata": {
        "id": "Me9-0cwWinQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "import pprint\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths\n",
        "model_path = \"/content/drive/MyDrive/Final Working LLM Code/Llama-2-FT.gguf\"\n",
        "dataset_path1 = \"/content/drive/MyDrive/Final Working LLM Code/Employee data 1.csv\"\n",
        "dataset_path2 = \"/content/drive/MyDrive/Final Working LLM Code/Employee data 2.csv\"\n",
        "schema_file_path = \"dataset_schema.json\"\n",
        "\n",
        "# Load the datasets from CSV files\n",
        "df1 = pd.read_csv(dataset_path1)\n",
        "df2 = pd.read_csv(dataset_path2)\n",
        "\n",
        "# Define the schemas with detailed descriptions\n",
        "schemas = {\n",
        "    \"tables\": {\n",
        "        \"EmployeeData1\": {\n",
        "            \"description\": \"Table containing basic employee information\",\n",
        "            \"columns\": {\n",
        "                \"Age\": {\n",
        "                    \"type\": \"integer\",\n",
        "                    \"description\": \"Age of the employee in years\"\n",
        "                },\n",
        "                \"Gender\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Gender of the employee\"\n",
        "                },\n",
        "                \"Education_Level\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Highest education level attained by the employee\"\n",
        "                },\n",
        "                \"Job_Title\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Job title of the employee\"\n",
        "                },\n",
        "                \"Years_of_Experience\": {\n",
        "                    \"type\": \"integer\",\n",
        "                    \"description\": \"Number of years the employee has worked\"\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"EmployeeData2\": {\n",
        "            \"description\": \"Table containing employee salary and name information\",\n",
        "            \"columns\": {\n",
        "                \"Salary\": {\n",
        "                    \"type\": \"float\",\n",
        "                    \"description\": \"Annual salary of the employee\"\n",
        "                },\n",
        "                \"EmpID\": {\n",
        "                    \"type\": \"integer\",\n",
        "                    \"description\": \"Unique identifier for the employee\"\n",
        "                },\n",
        "                \"FirstName\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"First name of the employee\"\n",
        "                },\n",
        "                \"LastName\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Last name of the employee\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Print the defined schemas\n",
        "print(\"EmployeeData1 table schema:\")\n",
        "print(json.dumps(schemas[\"tables\"][\"EmployeeData1\"], indent=4))\n",
        "print(\"\\nEmployeeData2 table schema:\")\n",
        "print(json.dumps(schemas[\"tables\"][\"EmployeeData2\"], indent=4))\n",
        "\n",
        "# Save the schemas to a JSON file\n",
        "with open(schema_file_path, 'w') as f:\n",
        "    json.dump(schemas, f)\n",
        "\n",
        "print(\"Dataset schemas saved to 'dataset_schema.json'\")\n",
        "\n",
        "# Load schema information from JSON file\n",
        "with open(schema_file_path, \"r\") as f:\n",
        "    schema_dict = json.load(f)\n",
        "\n",
        "# Define SQL schemas based on dataset variables\n",
        "sql_schema = \"\"\"\n",
        "CREATE TABLE EmployeeData1 (\n",
        "    Age INTEGER,\n",
        "    Gender TEXT,\n",
        "    Education_Level TEXT,\n",
        "    Job_Title TEXT,\n",
        "    Years_of_Experience INTEGER\n",
        ");\n",
        "\n",
        "CREATE TABLE EmployeeData2 (\n",
        "    Salary FLOAT,\n",
        "    EmpID INTEGER,\n",
        "    FirstName TEXT,\n",
        "    LastName TEXT\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "# User prompt with more detailed instructions\n",
        "user_prompt = \"Generate a SQL query to list the First names and last names of the people who have done PhD using joins function from EmployeeData1 and EmployeeData2 tables\"\n",
        "\n",
        "# Initialize LLM\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    chat_format=\"chatml\"\n",
        ")\n",
        "\n",
        "# Generate response with schema information\n",
        "response = llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant trained to generate SQL queries based on the given database schemas and user prompts.\"},\n",
        "        {\"role\": \"system\", \"content\": f\"Schema: {json.dumps(schemas)}\"},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "        {\"role\": \"assistant\", \"content\": \"Please provide the SQL query based on the provided schemas and prompt, using the table names and the correct column names from the schemas.\"}\n",
        "    ],\n",
        "    response_format={\"type\": \"text\"},\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "# Extract and print the generated SQL query\n",
        "generated_query = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "print(\"Generated SQL query:\")\n",
        "print(generated_query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "319ohXQPiyYW",
        "outputId": "0380b5b4-c172-43b3-eef5-c3fbc7262cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "EmployeeData1 table schema:\n",
            "{\n",
            "    \"description\": \"Table containing basic employee information\",\n",
            "    \"columns\": {\n",
            "        \"Age\": {\n",
            "            \"type\": \"integer\",\n",
            "            \"description\": \"Age of the employee in years\"\n",
            "        },\n",
            "        \"Gender\": {\n",
            "            \"type\": \"string\",\n",
            "            \"description\": \"Gender of the employee\"\n",
            "        },\n",
            "        \"Education_Level\": {\n",
            "            \"type\": \"string\",\n",
            "            \"description\": \"Highest education level attained by the employee\"\n",
            "        },\n",
            "        \"Job_Title\": {\n",
            "            \"type\": \"string\",\n",
            "            \"description\": \"Job title of the employee\"\n",
            "        },\n",
            "        \"Years_of_Experience\": {\n",
            "            \"type\": \"integer\",\n",
            "            \"description\": \"Number of years the employee has worked\"\n",
            "        }\n",
            "    }\n",
            "}\n",
            "\n",
            "EmployeeData2 table schema:\n",
            "{\n",
            "    \"description\": \"Table containing employee salary and name information\",\n",
            "    \"columns\": {\n",
            "        \"Salary\": {\n",
            "            \"type\": \"float\",\n",
            "            \"description\": \"Annual salary of the employee\"\n",
            "        },\n",
            "        \"EmpID\": {\n",
            "            \"type\": \"integer\",\n",
            "            \"description\": \"Unique identifier for the employee\"\n",
            "        },\n",
            "        \"FirstName\": {\n",
            "            \"type\": \"string\",\n",
            "            \"description\": \"First name of the employee\"\n",
            "        },\n",
            "        \"LastName\": {\n",
            "            \"type\": \"string\",\n",
            "            \"description\": \"Last name of the employee\"\n",
            "        }\n",
            "    }\n",
            "}\n",
            "Dataset schemas saved to 'dataset_schema.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /content/drive/MyDrive/Final Working LLM Code/Llama-2-FT.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q8_0:  226 tensors\n",
            "llm_load_vocab: special tokens cache size = 259\n",
            "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  6828.64 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
            "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
            "\n",
            "llama_print_timings:        load time =   57439.62 ms\n",
            "llama_print_timings:      sample time =      16.30 ms /    26 runs   (    0.63 ms per token,  1595.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =   57439.29 ms /   146 tokens (  393.42 ms per token,     2.54 tokens per second)\n",
            "llama_print_timings:        eval time =   21779.98 ms /    25 runs   (  871.20 ms per token,     1.15 tokens per second)\n",
            "llama_print_timings:       total time =   79256.35 ms /   171 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated SQL query:\n",
            "SELECT first_name FROM employee_data1 JOIN last_name FROM employee_data2 WHERE phd = \"yes\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IRIzJ3zlkkT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OuziNCjIUKkN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}